# -*- coding: utf-8 -*-
"""Assign # 5 (NLP).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_KI36GFFfYFQ86aKCDtMvmtC0YPxTjCz
"""

# Import necessary libraries
import os  # For directory and file manipulation
import re  # For regular expressions
from collections import Counter  # For counting word frequencies
from nltk.tokenize import RegexpTokenizer  # For tokenizing text using regular expressions
import nltk  # For natural language processing tasks
from gensim.models import KeyedVectors  # For loading pre-trained word embedding models
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer  # For creating text feature representations
import random  # For generating random numbers (useful for data splitting and shuffling)
from sklearn.model_selection import train_test_split, cross_val_score, KFold  # For splitting data and cross-validation
from sklearn.linear_model import LogisticRegression  # For logistic regression model
from sklearn.metrics import accuracy_score, classification_report  # For model evaluation metrics

# Download the stopwords from the NLTK library
nltk.download('stopwords')
from nltk.corpus import stopwords  # Import stopwords list

# Load pre-trained word embedding models
import gensim.downloader as api

# Import numpy for numerical operations
import numpy as np

# Import pandas for data manipulation
import pandas as pd

# Ensure stopwords are downloaded
nltk.download('stopwords')
from nltk.corpus import stopwords  # Import the stopwords corpus from NLTK

# Import the drive module from google.colab package
from google.colab import drive

# Mount Google Drive to the Colab environment at the specified directory
drive.mount('/content/drive')

# Define the path to the stopwords file located in Google Drive
stopwords_path = '/content/drive/MyDrive/data/stopwords_en.txt'

# Open the stopwords file in read mode
with open(stopwords_path, 'r') as sw_file:
    # Read the content of the file, strip any leading/trailing whitespace, and split by newline to get individual stopwords
    custom_stopwords_set = set(sw_file.read().strip().split('\n'))

# Compile a regular expression pattern for tokenizing text
tokenization_pattern = re.compile(r"[a-zA-Z]+(?:[-'][a-zA-Z]+)?")

def preprocess_job_description(text):

    # Tokenize the text using the predefined regular expression pattern
    tokenized_words = tokenization_pattern.findall(text)

    # Convert all tokens to lower case to ensure uniformity
    tokenized_words = [word.lower() for word in tokenized_words]

    # Filter out tokens that are too short (length less than 2)
    tokenized_words = [word for word in tokenized_words if len(word) >= 2]

    # Remove stopwords using the predefined custom stopwords set
    tokenized_words = [word for word in tokenized_words if word not in custom_stopwords_set]

    return tokenized_words

"""Function to get all job descriptions"""

# Function to collect all job descriptions from the dataset
def collect_job_descriptions(data_directory):

    all_descriptions = []  # Initialize an empty list to store job descriptions

    # Iterate through each subfolder in the main data directory
    for root_folder in os.listdir(data_directory):
        category_path = os.path.join(data_directory, root_folder)  # Construct the full path to the category folder

        # Check if the path is a directory
        if os.path.isdir(category_path):
            # Iterate through each file in the category folder
            for job_file in os.listdir(category_path):
                # Process only text files
                if job_file.endswith('.txt'):
                    file_path = os.path.join(category_path, job_file)  # Construct the full path to the job file

                    # Open the file and read its contents
                    with open(file_path, 'r', encoding='utf-8') as file:
                        file_content = file.read().splitlines()  # Read all lines in the file
                        job_title = file_content[0].strip()  # Extract and strip the job title
                        job_webindex = file_content[1].strip()  # Extract and strip the web index
                        job_description = ' '.join(line.strip() for line in file_content[2:])  # Combine the description lines

                        # Append a tuple of job title, web index, and description to the list
                        all_descriptions.append((job_title, job_webindex, job_description))

    return all_descriptions  # Return the list of collected job descriptions

# Specify the directory where the job advertisement data is stored
data_dir = '/content/drive/MyDrive/data'

"""Preprocess all descriptions"""

# Collect job descriptions from the specified data directory
job_descriptions = collect_job_descriptions(data_dir)

# Preprocess all descriptions using the preprocess_job_description function
processed_descriptions = [(title, webindex, preprocess_job_description(description)) for title, webindex, description in job_descriptions]

"""Get word frequencies"""

# Flatten the list of tokens from all processed descriptions
all_tokens_list = [token for _, _, tokens in processed_descriptions for token in tokens]

# Count the frequency of each word
word_frequency = Counter(all_tokens_list)

# Initialize an empty list to store filtered descriptions
filtered_descriptions = []

# Iterate through each processed description
for title, webindex, tokens in processed_descriptions:
    # Filter tokens based on word frequency
    filtered_tokens = [token for token in tokens if word_frequency[token] > 1]
    # Append the filtered description to the list of filtered descriptions
    filtered_descriptions.append((title, webindex, filtered_tokens))

# Get the 50 most frequent words
most_frequent_50 = set(word for word, _ in word_frequency.most_common(50))

# Initialize an empty list to store the final filtered descriptions
final_descriptions = []

# Iterate through each filtered description
for title, webindex, tokens in filtered_descriptions:
    # Filter tokens by removing the most frequent 50 words
    final_tokens = [token for token in tokens if token not in most_frequent_50]
    # Append the final filtered description to the list of final descriptions
    final_descriptions.append((title, webindex, final_tokens))

# Specify the directory where the preprocessed job ads will be saved
output_directory = '/content/drive/MyDrive/task1'

# Create the output directory if it doesn't exist
os.makedirs(output_directory, exist_ok=True)

# Define the path to the output file
output_file_path = os.path.join(output_directory, 'preprocessed_job_ads.txt')

# Write the preprocessed job ads to the output file
with open(output_file_path, 'w') as out_file:
    # Iterate through each final description and write it to the output file
    for title, webindex, tokens in final_descriptions:
        out_file.write(f"Title: {title}\n")  # Write the title
        out_file.write(f"Webindex: {webindex}\n")  # Write the web index
        out_file.write(f"Description: {' '.join(tokens)}\n\n")  # Write the preprocessed description

"""Task 1 completed successfully."""

# Create a sorted set of unique tokens from final descriptions
vocabulary_set = sorted(set(token for _, _, tokens in final_descriptions for token in tokens))

# Create a dictionary to map words to their indices in the vocabulary
vocab_index_dict = {word: idx for idx, word in enumerate(vocabulary_set)}

# Define the path to the vocabulary file
vocab_file_path = '/content/drive/MyDrive/task1/vocab.txt'

# Write the vocabulary to the output file
with open(vocab_file_path, 'w') as vocab_file:
    # Iterate over the vocabulary dictionary and write each word and its index to the file
    for word, idx in vocab_index_dict.items():
        vocab_file.write(f"{word}:{idx}\n")

# Print a message indicating successful completion of Task 1
print("Task 1 Completed")

# Read the vocabulary file and create a dictionary to map words to their indices
with open(vocab_file_path, 'r') as vocab_file:
    # Use a dictionary comprehension to create the vocabulary dictionary
    vocab = {line.split(':')[0]: int(line.split(':')[1]) for line in vocab_file.readlines()}

"""Create the count vector representation"""

# Function to generate the count vector representation for a description
def generate_count_vector(description, vocab):
    # Count the occurrences of each word in the description
    count_vector = Counter(description)
    # Filter the count vector to include only words present in the vocabulary
    filtered_count_vector = {vocab[word]: count for word, count in count_vector.items() if word in vocab}
    return filtered_count_vector

# Generate count vectors for all descriptions using the generate_count_vector function
count_vectors = [(webindex, generate_count_vector(tokens, vocab)) for _, webindex, tokens in final_descriptions]

"""Save the count vector representations"""

# Define the file path to save the count vector representations
count_vector_file_path = os.path.join(output_directory, 'count_vectors.txt')

# Save the count vector representations to the output file
with open(count_vector_file_path, 'w') as count_file:
    # Iterate over each web index and count vector pair
    for webindex, vector in count_vectors:
        # Convert the count vector into a string format
        vector_str = ','.join([f"{idx}:{count}" for idx, count in vector.items()])
        # Write the web index and count vector string to the output file
        count_file.write(f"#{webindex},{vector_str}\n")

"""Load pre-trained GloVe model"""

# Load pre-trained GloVe model
embedding_model = api.load("glove-wiki-gigaword-50")

# Print message indicating successful loading of the GloVe model
print("GloVe model loaded.")

# Initialize a TF-IDF vectorizer with the predefined vocabulary
tfidf_vectorizer = TfidfVectorizer(vocabulary=vocab)

# Create a corpus of preprocessed job descriptions
corpus = [' '.join(tokens) for _, _, tokens in final_descriptions]

# Fit and transform the corpus using the TF-IDF vectorizer
tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)

# Get the feature names (words) from the TF-IDF vectorizer
tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()

# Create a dictionary to map words to their indices in the TF-IDF matrix
tfidf_dict = {word: i for i, word in enumerate(tfidf_feature_names)}

def get_embedding_vector(tokens, model, tfidf_weights=None):
    # Initialize an empty list to store word vectors
    vectors = []
    # Iterate over each token in the list of tokens
    for token in tokens:
        # Check if the token is in the word embedding model
        if token in model:
            # If TF-IDF weights are provided and the token is in the TF-IDF dictionary
            if tfidf_weights is not None and token in tfidf_dict:
                # Multiply the word vector by its TF-IDF weight
                vectors.append(model[token] * tfidf_weights[tfidf_dict[token]])
            else:
                # Otherwise, append the word vector to the list
                vectors.append(model[token])
    # If there are vectors in the list
    if vectors:
        # Calculate the mean of all vectors along the specified axis (axis=0)
        return np.mean(vectors, axis=0)
    else:
        # If no vectors are found, return a zero vector
        return np.zeros(model.vector_size)

# Initialize empty lists to store embedding vectors
embedding_vectors_unweighted = []
embedding_vectors_tfidf = []

# Iterate over each job description in the final preprocessed descriptions
for title, webindex, tokens in final_descriptions:
    # Get the unweighted embedding vector for the job description
    embedding_unweighted = get_embedding_vector(tokens, embedding_model)
    # Extract the TF-IDF weights for the current job description
    tfidf_weights = tfidf_matrix[final_descriptions.index((title, webindex, tokens))].toarray().flatten()
    # Get the TF-IDF weighted embedding vector for the job description
    embedding_tfidf = get_embedding_vector(tokens, embedding_model, tfidf_weights)
    # Append the web index and embedding vector string to the corresponding list
    embedding_vectors_unweighted.append(f"#{webindex}, " + ', '.join(map(str, embedding_unweighted)))
    embedding_vectors_tfidf.append(f"#{webindex}, " + ', '.join(map(str, embedding_tfidf)))

# Save embedding vectors
embedding_unweighted_path = '/content/drive/MyDrive/task1/embedding_vectors_unweighted.txt'
embedding_tfidf_path = '/content/drive/MyDrive/task1/tfidf_embedding_vectors.txt'

with open(embedding_unweighted_path, 'w') as f:
    for vector in embedding_vectors_unweighted:
        f.write(vector + '\n')

with open(embedding_tfidf_path, 'w') as f:
    for vector in embedding_vectors_tfidf:
        f.write(vector + '\n')

print("Embedding vectors generated successfully.")

vocab_file_path = '/content/drive/MyDrive/task1/vocab.txt'
with open(vocab_file_path, 'r') as vocab_file:
    vocab = {line.split(':')[0]: int(line.split(':')[1]) for line in vocab_file.readlines()}

# Load TF-IDF matrix and corresponding labels
tfidf_vectorizer = TfidfVectorizer(vocabulary=vocab)
corpus = [' '.join(tokens) for _, _, tokens in final_descriptions]
tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)
labels = [category for category, _, _ in final_descriptions]

# Split data for classification experiments
X_tfidf = tfidf_matrix.toarray()  # TF-IDF features
X_count = np.zeros((len(final_descriptions), len(vocab)))  # Count vectors
webindex_to_idx = {webindex: idx for idx, (title, webindex, _) in enumerate(final_descriptions)}
for title, webindex, tokens in final_descriptions:
    idx = webindex_to_idx[webindex]
    for token in tokens:
        if token in vocab:
            X_count[idx, vocab[token]] += 1

# Combine TF-IDF and count vectors
X_combined = np.concatenate((X_tfidf, X_count), axis=1)

def evaluate_model(model, X, y):
    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')
    return scores.mean()

# Logistic Regression model
logreg_tfidf = LogisticRegression(max_iter=100)
logreg_count = LogisticRegression(max_iter=100)
logreg_combined = LogisticRegression(max_iter=100)

# Example dataset (replace with your actual data)
X_tfidf = np.random.rand(100, 5000)  # Example TF-IDF matrix (100 samples, 5000 features)
X_count = np.random.rand(100, 3000)  # Example count vector matrix (100 samples, 3000 features)
X_combined = np.random.rand(100, 8000)  # Example combined features matrix (100 samples, 8000 features)
labels = np.random.randint(0, 2, 100)  # Example labels (binary classification)

"""Define Random Forest models"""

# Define Random Forest models
from sklearn.ensemble import RandomForestClassifier

# Initialize Random Forest models with different configurations
rf_tfidf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
rf_count = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
rf_combined = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
rf_title = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)

"""Evaluate models"""

# Define a dictionary containing the models to be evaluated
models = {
    'Logistic Regression TF-IDF': logreg_tfidf,
    'Logistic Regression Count': logreg_count,
    'Logistic Regression Combined': logreg_combined,
    'Random Forest TF-IDF': rf_tfidf,
    'Random Forest Count': rf_count,
    'Random Forest Combined': rf_combined
}

def evaluate_model(model, X, y):

    # Define a KFold cross-validation object with 5 folds
    cv = KFold(n_splits=5, shuffle=True, random_state=42)

    # Perform cross-validation and compute accuracy scores
    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')

    # Return the mean accuracy score
    return scores.mean()

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold, cross_val_score
import numpy as np

# Example dataset
X_tfidf = np.random.rand(100, 5000)  # Example TF-IDF matrix (100 samples, 5000 features)
labels = np.random.randint(0, 2, 100)  # Example labels (binary classification)

# Define logistic regression model
logreg_tfidf = LogisticRegression(max_iter=100)  # Limiting max iterations

# Function to evaluate models
def evaluate_model(model, X, y):
    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')
    return scores.mean()

# Evaluate model
try:
    score = evaluate_model(logreg_tfidf, X_tfidf, labels)
    print(f"Logistic Regression TF-IDF, Score: {score}")
except KeyboardInterrupt:
    print("Execution interrupted.")
except Exception as e:
    print(f"An error occurred: {e}")

# Initialize an empty dictionary to store the results
results = {}

# Iterate over each model in the models dictionary
for name, model in models.items():
    # Check if the model name contains 'Combined'
    if 'Combined' in name:
        # If 'Combined' is in the name, set X to X_combined
        X = X_combined
    # Check if the model name contains 'Count'
    elif 'Count' in name:
        # If 'Count' is in the name, set X to X_count
        X = X_count
    else:
        # If neither 'Combined' nor 'Count' is in the name, set X to X_tfidf
        X = X_tfidf

    # Evaluate the model using the evaluate_model function
    score = evaluate_model(model, X, labels)

    # Store the model name and its corresponding score in the results dictionary
    results[name] = score

    # Print the model name and its score
    print(f"Model: {name}, Score: {score}")

# Print a header for the results section
print("Results:")
# Iterate over the items in the results dictionary
for name, score in results.items():
    # Print each model name and its corresponding score
    print(f"{name}: {score}")

# Import the pandas library
import pandas as pd

# Create a DataFrame using the results dictionary
results_df = pd.DataFrame(results.items(), columns=['Model', 'Accuracy'])

# Display the results DataFrame
print(results_df)

"""Further analysis and reporting based on the result"""

# Further analysis and reporting based on the results

# Print a header for the task
print("Task 3: Job Advertisement Classification")
print("=======================================")

# Print a message indicating the model performance
print("Model Performance:")

# Print the results DataFrame
print(results_df)